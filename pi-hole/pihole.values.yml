# Default values for pihole.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# -- The number of replicas
replicaCount: 3

# -- The `spec.strategyTpye` for updates
strategyType: RollingUpdate

# -- The maximum number of Pods that can be created over the desired number of `ReplicaSet` during updating.
maxSurge: 1

# -- The maximum number of Pods that can be unavailable during updating
maxUnavailable: 1

image:
  # -- the repostory to pull the image from
  repository: "pihole/pihole"
  # -- the docker tag, if left empty it will get it from the chart's appVersion
  tag: "2023.05.1"
  # -- the pull policy
  pullPolicy: IfNotPresent

dualStack:
  # -- set this to true to enable creation of DualStack services or creation of separate IPv6 services if `serviceDns.type` is set to `"LoadBalancer"`
  enabled: false

dnsHostPort:
  # -- set this to true to enable dnsHostPort
  enabled: true
  # -- default port for this pod
  port: 53

# -- Configuration for the DNS service on port 53
serviceDns:

  # -- deploys a mixed (TCP + UDP) Service instead of separate ones
  mixedService: false

  # -- `spec.type` for the DNS Service
  type: NodePort

  # -- The port of the DNS service
  port: 53

  # -- Optional node port for the DNS service
  nodePort: ""

  # -- `spec.externalTrafficPolicy` for the DHCP Service
  externalTrafficPolicy: Local

  # -- A fixed `spec.loadBalancerIP` for the DNS Service
  loadBalancerIP: ""
  # -- A fixed `spec.loadBalancerIP` for the IPv6 DNS Service
  loadBalancerIPv6: ""

  # -- Annotations for the DNS service
  annotations: {}
    # metallb.universe.tf/address-pool: network-services
    # metallb.universe.tf/allow-shared-ip: pihole-svc

# -- Configuration for the DHCP service on port 67
serviceDhcp:

  # -- Generate a Service resource for DHCP traffic
  enabled: true

  # -- `spec.type` for the DHCP Service
  type: NodePort

  # -- `spec.externalTrafficPolicy` for the DHCP Service
  externalTrafficPolicy: Local

  # -- A fixed `spec.loadBalancerIP` for the DHCP Service
  loadBalancerIP: ""
  # -- A fixed `spec.loadBalancerIP` for the IPv6 DHCP Service
  loadBalancerIPv6: ""

  # -- Annotations for the DHCP service
  annotations: {}
    # metallb.universe.tf/address-pool: network-services
    # metallb.universe.tf/allow-shared-ip: pihole-svc

# -- Configuration for the web interface service
serviceWeb:
  # -- Configuration for the HTTP web interface listener
  http:

    # -- Generate a service for HTTP traffic
    enabled: true

    # -- The port of the web HTTP service
    port: 80

  # -- Configuration for the HTTPS web interface listener
  https:
    # -- Generate a service for HTTPS traffic
    enabled: true

    # -- The port of the web HTTPS service
    port: 443

  # -- `spec.type` for the web interface Service
  type: ClusterIP

  # -- `spec.externalTrafficPolicy` for the web interface Service
  externalTrafficPolicy: Local

  # -- A fixed `spec.loadBalancerIP` for the web interface Service
  loadBalancerIP: ""
  # -- A fixed `spec.loadBalancerIP` for the IPv6 web interface Service
  loadBalancerIPv6: ""

  # -- Annotations for the DHCP service
  annotations:
    traefik.ingress.kubernetes.io/service.sticky.cookie: "true"
    traefik.ingress.kubernetes.io/service.sticky.cookie.name: "pihole"
    # metallb.universe.tf/address-pool: network-services
    # metallb.universe.tf/allow-shared-ip: pihole-svc

virtualHost: pi.hole

# -- Configuration for the Ingress
ingress:
  # -- Generate a Ingress resource
  enabled: true

  # -- Specify an ingressClassName
  # ingressClassName: nginx

  # -- Annotations for the ingress
  annotations:
    kubernetes.io/ingress.class: traefik
    cert-manager.io/cluster-issuer: letsencrypt-prod
    traefik.ingress.kubernetes.io/router.middlewares: cert-manager-redirect-https@kubernetescrd

  path: /
  hosts:
    # virtualHost (default value is pi.hole) will be appended to the hosts
    - pihole.k3s.yorgos.net.gr
  tls:
    - secretName: pihole-tls
      hosts:
        - pihole.k3s.yorgos.net.gr
  #  - secretName: chart-example-tls
  #    hosts:
  #     #- virtualHost (default value is pi.hole) will be appended to the hosts
  #      - chart-example.local

# -- Probes configuration
probes:
  # -- probes.liveness -- Configure the healthcheck for the ingress controller
  liveness:
    # -- Generate a liveness probe
    enabled: true
    initialDelaySeconds: 60
    failureThreshold: 10
    timeoutSeconds: 5
  readiness:
    # -- Generate a readiness probe
    enabled: true
    initialDelaySeconds: 60
    failureThreshold: 3
    timeoutSeconds: 5

# -- We usually recommend not to specify default resources and to leave this as a conscious
# -- choice for the user. This also increases chances charts run on environments with little
# -- resources, such as Minikube. If you do want to specify resources, uncomment the following
# -- lines, adjust them as necessary, and remove the curly braces after 'resources:'.
resources: {}
  # limits:
  #  cpu: 100m
  #  memory: 128Mi
  # requests:
  #  cpu: 100m
  #  memory: 128Mi

# -- `spec.PersitentVolumeClaim` configuration
persistentVolumeClaim:
  # -- set to true to use pvc
  enabled: true

  # -- specify an existing `PersistentVolumeClaim` to use
  # existingClaim: ""

  # -- Annotations for the `PersitentVolumeClaim`
  annotations: {}

  accessModes:
    - ReadWriteOnce

  size: "500Mi"

  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  storageClass: "managed-nfs-storage"

  ## If subPath is set mount a sub folder of a volume instead of the root of the volume.
  ## This is especially handy for volume plugins that don't natively support sub mounting (like glusterfs).

  ## subPath: "pihole"

nodeSelector: {}

tolerations: []

# -- Specify a priorityClassName
# priorityClassName: ""

# Reference: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
topologySpreadConstraints: []
# - maxSkew: <integer>
#   topologyKey: <string>
#   whenUnsatisfiable: <string>
#   labelSelector: <object>

affinity: {}

# -- Administrator password when not using an existing secret (see below)
adminPassword: "admin"

# -- Use an existing secret for the admin password.
admin:
  # -- Specify an existing secret to use as admin password
  existingSecret: "pihole-secret"
  # -- Specify the key inside the secret to use
  passwordKey: "password"

# -- extraEnvironmentVars is a list of extra enviroment variables to set for pihole to use
extraEnvVars:
  TZ: Europe/Athens
  DNSMASQ_USER: root
  PIHOLE_UID: 0

# -- extraEnvVarsSecret is a list of secrets to load in as environment variables.
extraEnvVarsSecret: {}
  # env_var:
  #   name: secret-name
  #   key: secret-key

# -- default upstream DNS 1 server to use
DNS1: "1.1.1.1"
# -- default upstream DNS 2 server to use
DNS2: "8.8.8.8"

antiaff:
  # -- set to true to enable antiaffinity (example: 2 pihole DNS in the same cluster)
  enabled: true
  # -- Here you can set the pihole release (you set in `helm install <releasename> ...`)
  # you want to avoid
  avoidRelease: pihole1
  # -- Here you can choose between preferred or required
  strict: true

doh:
  # -- set to true to enabled DNS over HTTPs via cloudflared
  enabled: false
  name: "cloudflared"
  repository: "crazymax/cloudflared"
  tag: latest
  pullPolicy: IfNotPresent
  # -- Here you can pass environment variables to the DoH container, for example:
  envVars: {}
    # TUNNEL_DNS_UPSTREAM: "https://1.1.1.2/dns-query,https://1.0.0.2/dns-query"

  # -- Probes configuration
  probes:
    # -- Configure the healthcheck for the doh container
    liveness:
      # -- set to true to enable liveness probe
      enabled: true
      # -- defines the initial delay for the liveness probe
      initialDelaySeconds: 60
      # -- defines the failure threshold for the liveness probe
      failureThreshold: 10
      # -- defines the timeout in secondes for the liveness probe
      timeoutSeconds: 5

dnsmasq:
  # -- Add upstream dns servers. All lines will be added to the pihole dnsmasq configuration
  upstreamServers: []
  # - server=/foo.bar/192.168.178.10
  # - server=/bar.foo/192.168.178.11

  # -- Add custom dns entries to override the dns resolution. All lines will be added to the pihole dnsmasq configuration.
  customDnsEntries:
#   - address=/node1.k3s.yorgos.net.gr/192.168.100.180
#   - address=/node2.k3s.yorgos.net.gr/192.168.100.181
#   - address=/node3.k3s.yorgos.net.gr/192.168.100.183
#   - address=/k3s.yorgos.net.gr/192.168.100.180
#   - address=/k3s.yorgos.net.gr/192.168.100.181
#   - address=/k3s.yorgos.net.gr/192.168.100.183
   - address=/PlayRoom.kalimera.yorgos.net.gr/fe80::72f7:54ff:fedb:280d
   - address=/PlayRoom.kalimera.yorgos.net.gr/192.168.50.19
   - ptr-record=lb._dns-sd._udp.kalimera.yorgos.net.gr,kalimera.yorgos.net.gr
   - ptr-record=b._dns-sd._udp.kalimera.yorgos.net.gr,kalimera.yorgos.net.gr
   - ptr-record=D.0.8.2.B.D.E.F.F.F.4.5.7.F.2.7.0.0.0.0.0.0.0.0.0.0.0.0.0.0.8.E.F.ip6.arpa,PlayRoom.kalimera.yorgos.net.gr
   - ptr-record=19.50.168.192.in-addr.arpa,PlayRoom.kalimera.yorgos.net.gr
   - srv-host=PlayRoom._linkplay._tcp.kalimera.yorgos.net.gr,PlayRoom.kalimera.yorgos.net.gr,59152,0,0
   - ptr-record=_linkplay._tcp.kalimera.yorgos.net.gr,PlayRoom._linkplay._tcp.kalimera.yorgos.net.gr
   - ptr-record=_services._dns-sd._udp.kalimera.yorgos.net.gr,_linkplay._tcp.kalimera.yorgos.net.gr
   - txt-record=PlayRoom._linkplay._tcp.kalimera.yorgos.net.gr,"uuid=uuid:FF98F08F-E4BE-B85C-4A86-51ECFF98F08F" "MAC=70:F7:54:DB:28:0D" "security=https 3.0" "upnp=1.0.0" "bootid=ba0e9102-1dd1-11b2-87c3-b9b174a8aede"
   - srv-host=TidalConnect-8726f3cd2e6833464288279eb61880fc._tidalconnect._tcp.kalimera.yorgos.net.gr,PlayRoom.kalimera.yorgos.net.gr,2019,0,0
   - ptr-record=_tidalconnect._tcp.kalimera.yorgos.net.gr,TidalConnect-8726f3cd2e6833464288279eb61880fc._tidalconnect._tcp.kalimera.yorgos.net.gr
   - ptr-record=_services._dns-sd._udp.kalimera.yorgos.net.gr,_tidalconnect._tcp.kalimera.yorgos.net.gr
   - txt-record=TidalConnect-8726f3cd2e6833464288279eb61880fc._tidalconnect._tcp.kalimera.yorgos.net.gr,"mn=TidalConnect" "ca=0" "id=8726f3cd2e6833464288279eb61880fc" "ve=1" "fn=PlayRoom"
   - srv-host=70F754DB280D@PlayRoom._raop._tcp.kalimera.yorgos.net.gr,PlayRoom.kalimera.yorgos.net.gr,7000,0,0
   - srv-host=PlayRoom._airplay._tcp.kalimera.yorgos.net.gr,PlayRoom.kalimera.yorgos.net.gr,7000,0,0
   - srv-host=Chromecast._http._tcp.kalimera.yorgos.net.gr,PlayRoom.kalimera.yorgos.net.gr,80,0,0
   - ptr-record=_raop._tcp.kalimera.yorgos.net.gr,70F754DB280D@PlayRoom._raop._tcp.kalimera.yorgos.net.gr
   - ptr-record=_services._dns-sd._udp.kalimera.yorgos.net.gr,_raop._tcp.kalimera.yorgos.net.gr
   - txt-record=70F754DB280D@PlayRoom._raop._tcp.kalimera.yorgos.net.gr,"cn=0,1" "da=true" "et=0,4" "ft=0x445F8A00,0x1C340" "fv=p20.1.56.430671" "md=0,1,2" "am=LINK 2 Wireless multiroom HiFi player" "sf=0x4" "tp=UDP" "vn=65537" "vs=366.0" "pk=46a5e24448ecf1ea5be076d583c7ba499d08efe074cf4221bd0491dd8d9350bd"
   - ptr-record=_airplay._tcp.kalimera.yorgos.net.gr,PlayRoom._airplay._tcp.kalimera.yorgos.net.gr
   - ptr-record=_services._dns-sd._udp.kalimera.yorgos.net.gr,_airplay._tcp.kalimera.yorgos.net.gr
   - txt-record=PlayRoom._airplay._tcp.kalimera.yorgos.net.gr,"acl=0" "deviceid=70:F7:54:DB:28:0D" "features=0x445F8A00,0x1C340" "rsf=0x0" "fv=p20.1.56.430671" "flags=0x4" "model=LINK 2 Wireless multiroom HiFi player" "manufacturer=Audio Pro AB" "serialNumber=1234567890" "protovers=1.1" "srcvers=366.0" "pi=70:F7:54:DB:28:0D" "gid=4FA8E02B-2998-412F-98FC-593341AC259B" "gcgl=0" "pk=46a5e24448ecf1ea5be076d583c7ba499d08efe074cf4221bd0491dd8d9350bd"
   - ptr-record=_http._tcp.kalimera.yorgos.net.gr,Chromecast._http._tcp.kalimera.yorgos.net.gr
   - ptr-record=_services._dns-sd._udp.kalimera.yorgos.net.gr,_http._tcp.kalimera.yorgos.net.gr
   - txt-record=Chromecast._http._tcp.kalimera.yorgos.net.gr,""


  # -- Dnsmasq reads the /etc/hosts file to resolve ips. You can add additional entries if you like
  additionalHostsEntries: []
  # - 192.168.0.3     host4
  # - 192.168.0.4     host5

  # -- Static DHCP config
  staticDhcpEntries: []
  # staticDhcpEntries:
  # - dhcp-host=MAC_ADDRESS,IP_ADDRESS,HOSTNAME

  # -- Other options
  customSettings:
  # otherSettings:
  # - rebind-domain-ok=/plex.direct/

  # -- Here we specify custom cname entries that should point to `A` records or
  # elements in customDnsEntries array.
  # The format should be:
  #  - cname=cname.foo.bar,foo.bar
  #  - cname=cname.bar.foo,bar.foo
  #  - cname=cname record,dns record
  customCnameEntries: []
  # Here we specify custom cname entries that should point to `A` records or
  # elements in customDnsEntries array.
  # The format should be:
  #   - cname=cname.foo.bar,foo.bar
  #   - cname=cname.bar.foo,bar.foo
  #   - cname=cname record,dns record

# -- list of adlists to import during initial start of the container
adlists: {}
  # If you want to provide blocklists, add them here.
  # - https://hosts-file.net/grm.txt
  # - https://reddestdream.github.io/Projects/MinimalHosts/etc/MinimalHostsBlocker/minimalhosts

# -- list of whitelisted domains to import during initial start of the container
whitelist: {}
  # If you want to provide whitelisted domains, add them here.
  # - clients4.google.com

# -- list of blacklisted domains to import during initial start of the container
blacklist: {}
  # If you want to have special domains blacklisted, add them here
  # - *.blackist.com

# -- list of blacklisted regex expressions to import during initial start of the container
regex: {}
  # Add regular expression blacklist items
  # - (^|\.)facebook\.com$

# -- values that should be added to pihole-FTL.conf
ftl: {}
  # Add values for pihole-FTL.conf
  # MAXDBDAYS: 14

# -- port the container should use to expose HTTP traffic
webHttp: "8881"

# -- port the container should use to expose HTTPS traffic
webHttps: "8483"

# -- hostname of pod
hostname: ""

# -- should the container use host network
hostNetwork: "true"

# -- should container run in privileged mode
privileged: "true"

customVolumes:
  # -- set this to true to enable custom volumes
  enabled: false
  # -- any volume type can be used here
  config: {}
    # hostPath:
    #   path: "/mnt/data"

# -- any extra volumes you might want
extraVolumes: {}
  # external-conf:
  #   configMap:
  #     name: pi-hole-lighttpd-external-conf

# -- any extra volume mounts you might want
extraVolumeMounts: {}
  # external-conf:
  #   mountPath: /etc/lighttpd/external.conf
  #   subPath: external.conf

# -- Additional annotations for pods
podAnnotations: {}
  # Example below allows Prometheus to scape on metric port (requires pihole-exporter sidecar enabled)
  # prometheus.io/port: '9617'
  # prometheus.io/scrape: 'true'

monitoring:
  # -- Preferably adding prometheus scrape annotations rather than enabling podMonitor.
  podMonitor:
    # -- set this to true to enable podMonitor
    enabled: false
  # -- Sidecar configuration
  sidecar:
    # -- set this to true to enable podMonitor as sidecar
    enabled: false
    port: 9617
    image:
      repository: ekofr/pihole-exporter
      tag: 0.0.10
      pullPolicy: IfNotPresent
    resources:
      limits:
        memory: 128Mi
      # requests:
      #  cpu: 100m
      #  memory: 128Mi

podDnsConfig:
  enabled: true
  policy: "None"
  nameservers:
  - 127.0.0.1
  - 8.8.8.8

